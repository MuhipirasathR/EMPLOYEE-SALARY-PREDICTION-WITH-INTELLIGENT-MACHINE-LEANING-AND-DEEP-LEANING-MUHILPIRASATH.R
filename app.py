# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cEM7-6lORYEDN-a49EvyJFUrGSICgDkR
"""

# Basic Libraries
import pandas as pd
import numpy as np

# Scikit-learn Metrics (assuming you need these here)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Joblib for model saving/loading
import joblib

# Although LogisticRegression is imported later, keeping it here for completeness if needed elsewhere
from sklearn.linear_model import LogisticRegression

# Scikit-learn Modules
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Machine Learning Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier

# Deep Learning (TensorFlow/Keras)
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Other relevant imports used in this section
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report # Import specific metrics
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import GradientBoostingClassifier
import joblib
import streamlit as st
import numpy as np
# Removed: redundant imports and the incomplete 'from sklearn.metrics'

df = pd.read_csv("adult3.csv")

# Define the target column
target_col = 'income_>50K'  # binary classification: 1 if >50K, 0 otherwise

X = df.drop('income', axis=1)
y = df['income']
# Convert target variable to binary
y = y.apply(lambda x: 1 if x == '>50K' else 0)
# Separate features and target before encoding
X = df.drop('income', axis=1)
y = df['income'].apply(lambda x: 1 if x == '>50K' else 0)

# One-hot encode categorical columns
X = pd.get_dummies(X, drop_first=True)
print("Shape after encoding:", X.shape)
print("Sample columns:\n", X.columns[:10])

from sklearn.model_selection import train_test_split

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Check shape
print("Train set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC(),
    "Naive Bayes": GaussianNB(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "XGBoost": XGBClassifier(eval_metric='logloss')
}

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

ml_results = {}

from sklearn.impute import SimpleImputer

# Impute missing values before scaling
imputer = SimpleImputer(strategy='median')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Scale the imputed data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_imputed)
X_test_scaled = scaler.transform(X_test_imputed)

ml_results = {}

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    ml_results[name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred, zero_division=0),
        "Recall": recall_score(y_test, y_pred, zero_division=0),
        "F1 Score": f1_score(y_test, y_pred, zero_division=0)
    }

# Display results
results_df = pd.DataFrame(ml_results).T.sort_values(by="Accuracy", ascending=False)
print("\nML Model Performance Comparison:\n")
print(results_df)

# Define the model
dl_model = Sequential()
dl_model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))
dl_model.add(Dropout(0.3))
dl_model.add(Dense(64, activation='relu'))
dl_model.add(Dropout(0.3))
dl_model.add(Dense(32, activation='relu'))
dl_model.add(Dense(1, activation='sigmoid'))  # Binary output

dl_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

history = dl_model.fit(
    X_train_scaled, y_train,
    epochs=20,
    batch_size=64,
    validation_data=(X_test_scaled, y_test),
    verbose=1
)

# Predict probabilities and convert to binary
y_pred_dl_prob = dl_model.predict(X_test_scaled)
y_pred_dl = (y_pred_dl_prob > 0.5).astype(int)

# Evaluate
print("\nDeep Learning Model Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_dl))
print("Precision:", precision_score(y_test, y_pred_dl))
print("Recall:", recall_score(y_test, y_pred_dl))
print("F1 Score:", f1_score(y_test, y_pred_dl))

import matplotlib.pyplot as plt
# Plot accuracy
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
# Plot loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Extract accuracy from ml_results
accuracy_results = {name: scores['Accuracy'] for name, scores in ml_results.items()}#hi mam i am MUHILPIRASATH R FROM SONA COLLEGE OF TECHNOLOGY
# Add DL accuracy manually
accuracy_results['Deep Learning'] = 0.8608864776333299
# Plot
plt.figure(figsize=(10, 6))
plt.bar(accuracy_results.keys(), accuracy_results.values(), color='skyblue')
plt.ylabel('Accuracy Score')
plt.title('Model Accuracy Comparison (ML vs DL)')
plt.xticks(rotation=45)
plt.ylim(0.75, 0.9)
plt.grid(True, axis='y')
plt.tight_layout()
plt.show()

# Find best ML model
best_model_name = max(ml_results, key=lambda x: ml_results[x]['Accuracy'])
best_model = models[best_model_name]
# Save best ML model
import joblib
joblib.dump(best_model, f'best_model_{best_model_name}.pkl')
print(f"Best ML model ({best_model_name}) saved as best_model_{best_model_name}.pkl")

accuracy_results = {
    "XGBoost": ml_results["XGBoost"]["Accuracy"],
    "Random Forest": ml_results["Random Forest"]["Accuracy"],
    "Logistic Regression": ml_results["Logistic Regression"]["Accuracy"],
    "Decision Tree": ml_results["Decision Tree"]["Accuracy"],
    "SVM": ml_results["SVM"]["Accuracy"],
    "Naive Bayes": ml_results["Naive Bayes"]["Accuracy"],
    "K-Nearest Neighbors": ml_results["K-Nearest Neighbors"]["Accuracy"],
    "Deep Learning":0.8608864776333299   # your deep learning model's accuracy
}

best_model_name = max(accuracy_results, key=accuracy_results.get)
print(f"Best performing model: {best_model_name} with accuracy: {accuracy_results[best_model_name]:.4f}")

if best_model_name == "Deep Learning":
    dl_model.save("best_model_deep_learning.h5")
    print("Deep Learning model saved as best_model_deep_learning.h5")
else:
    best_ml_model = models[best_model_name]
    best_ml_model.fit(X_train_scaled, y_train)
    joblib.dump(best_ml_model, f"best_model_{best_model_name}.pkl")
    print(f"{best_model_name} saved as best_model_{best_model_name}.pkl")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "RandomForest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "GradientBoosting": GradientBoostingClassifier()
}

from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

results = {}

# Define models (ensure this dictionary is correct and includes all models you want to evaluate)
models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "RandomForest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "GradientBoosting": GradientBoostingClassifier()
}


for name, model in models.items():
    # Create a pipeline with imputation, scaling, and the model
    pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='median')), # Impute missing values
        ('scaler', StandardScaler()),                 # Scale features
        ('classifier', model)                         # The actual model
    ])

    # Fit the pipeline on the training data
    pipeline.fit(X_train, y_train)

    # Make predictions on the test data
    y_pred = pipeline.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy

    print(f"\n{name} Accuracy: {accuracy:.4f}")
    print(classification_report(y_test, y_pred))

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report

results = {}

# Define models (ensure this dictionary is correct and includes all models you want to evaluate)
# Assuming 'models' dictionary is already defined and available in the environment
# models = {
#     "LogisticRegression": LogisticRegression(max_iter=1000),
#     "RandomForest": RandomForestClassifier(),
#     "KNN": KNeighborsClassifier(),
#     "SVM": SVC(),
#     "GradientBoosting": GradientBoostingClassifier()
# }


for name, model in models.items():
    # Create a pipeline with imputation, scaling, and the model
    pipe = Pipeline([
        ('imputer', SimpleImputer(strategy='median')), # Add imputation step
        ('scaler', StandardScaler()),
        ('model', model)
    ])
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"\n{name} Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))

# Get best model
best_model_name = max(results, key=results.get)
best_model = models[best_model_name]
print(f"\n‚úÖ Best model: {best_model_name} with accuracy {results[best_model_name]:.4f}")

# Save the best model
joblib.dump(best_model, "best_model.pkl")
print("‚úÖ Saved best model as best_model.pkl")

# Categorical and numerical columns
categorical_cols = ['education', 'occupation']
numerical_cols = ['age', 'hours-per-week', 'experience']

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingClassifier

# Categorical and numerical columns
# These lists are no longer needed after one-hot encoding
# categorical_cols = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']
numerical_cols = ['age', 'fnlwgt', 'educational-num', 'capital-gain', 'capital-loss', 'hours-per-week']

# Create preprocessor - No longer needed after initial encoding
# preprocessor = ColumnTransformer(
#     transformers=[
#         ('num', StandardScaler(), numerical_cols),
#         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
#     ],
#     remainder='passthrough' # Keep other columns (if any)
# )

# Final pipeline
# Apply StandardScaler directly to the numerical columns within the pipeline
model_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', GradientBoostingClassifier())
])

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingClassifier


# Split data (if not already done)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the pipeline with imputation, scaling, and the model
model_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')), # Add imputation step
    ('scaler', StandardScaler()),
    ('classifier', GradientBoostingClassifier())
])

# Train the pipeline
model_pipeline.fit(X_train, y_train)

# Make predictions
y_pred_pipeline = model_pipeline.predict(X_test)

# Evaluate the pipeline
accuracy_pipeline = accuracy_score(y_test, y_pred_pipeline)
print(f"Pipeline Accuracy: {accuracy_pipeline:.4f}")
print("\nPipeline Classification Report:")
print(classification_report(y_test, y_pred_pipeline))

# Define models
models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "RandomForest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "GradientBoosting": GradientBoostingClassifier()
}

results = {}

# Separate features and label
X = df.drop('income', axis=1)
y = df['income']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# # Load trained model
# model = joblib.load("model_lightgbm_v2.pkl")
# 
# st.set_page_config(page_title="Employee Salary Estimator", layout="centered")
# 
# # Custom CSS to style the app
# st.markdown("""
#     <style>
#     .main {
#         background-color: #f9f9f9;
#         padding: 2rem;
#         border-radius: 10px;
#         box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
#         font-family: 'Segoe UI', sans-serif;
#     }
#     h1 {
#         color: #1f77b4;
#         text-align: center;
#     }
#     .stButton > button {
#         background-color: #1f77b4;
#         color: white;
#         border-radius: 8px;
#         height: 3em;
#         width: 100%;
#         font-size: 16px;
#     }
#     .stSuccess {
#         background-color: #d4edda;
#         color: #155724;
#         padding: 1rem;
#         border-radius: 8px;
#         font-size: 18px;
#     }
#     </style>
# """, unsafe_allow_html=True)
# st.markdown("<div class='main'>", unsafe_allow_html=True)
# 
# st.title("üíº Employee Salary Estimator")
# st.write("Use the form below to estimate an employee's expected monthly salary.")
# st.markdown("---")
# 
# # Actual category labels
# workclass_options = ["Private", "Self-emp-not-inc", "Self-emp-inc", "Federal-gov", "Local-gov", "State-gov", "Without-pay", "Never-worked"]
# education_options = ["Bachelors", "HS-grad", "11th", "Masters", "9th", "Some-college", "Assoc-acdm", "Assoc-voc", "Doctorate", "7th-8th", "Prof-school", "5th-6th", "10th", "1st-4th", "Preschool", "12th"]
# marital_status_options = ["Married-civ-spouse", "Divorced", "Never-married", "Separated", "Widowed", "Married-spouse-absent", "Married-AF-spouse"]
# occupation_options = ["Tech-support", "Craft-repair", "Other-service", "Sales", "Exec-managerial", "Prof-specialty", "Handlers-cleaners", "Machine-op-inspct", "Adm-clerical", "Farming-fishing", "Transport-moving", "Priv-house-serv", "Protective-serv", "Armed-Forces", "Unknown"]
# relationship_options = ["Wife", "Own-child", "Husband", "Not-in-family", "Other-relative", "Unmarried"]
# race_options = ["White", "Black", "Asian-Pac-Islander", "Amer-Indian-Eskimo", "Other"]
# native_country_options = ["United-States", "Mexico", "Philippines", "Germany", "Canada", "India", "China", "Cuba", "England", "Japan", "South"]
# 
# # Collect user inputs
# age = st.slider("Age", 18, 90, 30)
# workclass = st.selectbox("Workclass", workclass_options)
# education = st.selectbox("Education", education_options)
# education_num = st.slider("Education Number", 1, 16, 9)
# marital_status = st.selectbox("Marital Status", marital_status_options)
# occupation = st.selectbox("Occupation", occupation_options)
# relationship = st.selectbox("Relationship", relationship_options)
# race = st.selectbox("Race", race_options)
# sex = st.selectbox("Sex", ["Female", "Male"])
# capital_gain = st.number_input("Capital Gain", 0)
# capital_loss = st.number_input("Capital Loss", 0)
# hours_per_week = st.slider("Hours Per Week", 1, 99, 40)
# native_country = st.selectbox("Native Country", native_country_options)
# fnlwgt = st.number_input("Final Weight (fnlwgt)", 0)
# 
# # Encode inputs
# workclass_encoded = workclass_options.index(workclass)
# education_encoded = education_options.index(education)
# marital_status_encoded = marital_status_options.index(marital_status)
# occupation_encoded = occupation_options.index(occupation)
# relationship_encoded = relationship_options.index(relationship)
# race_encoded = race_options.index(race)
# native_country_encoded = native_country_options.index(native_country)
# sex_encoded = 1 if sex == "Male" else 0
# 
# # Input feature order must match model training
# input_data = np.array([[
#     age, workclass_encoded, education_encoded, education_num, marital_status_encoded,
#     occupation_encoded, relationship_encoded, race_encoded, sex_encoded, capital_gain,
#     capital_loss, hours_per_week, native_country_encoded, fnlwgt
# ]])
# # Load trained model
# model = joblib.load("model_lightgbm_v2.pkl")
# 
# st.set_page_config(page_title="Salary Predictor | ML + DL", layout="wide")
# 
# # Developer Info and Header
# col1, col2 = st.columns([1, 3])
# with col1:
#     image = Image.open("muhil.jpg")  # <- Your uploaded photo
#     st.image(image, width=200)
# with col2:
#     st.markdown("""
#         ## üë®‚Äçüíº MUHILPIRASATH R
#         üéì **Mechatronics Engineering, Sona College of Technology (Batch 2027)**
#         üìå **Internship Project:** *Employee Salary Prediction using ML & DL*
#         üè¢ *Edunet Foundation | AICTE | IBM*
#         üïê *Duration:* 6 Weeks
#         üß† *Passionate about AI/ML, Robotics, Automation*
#     """)
# 
# st.markdown("---")
# 
# # Project Theme and Mode
# with st.expander("üé® Project Theme"):
#     st.info("""
#     - **Design:** 3D Green Matrix Theme
#     - **Modes:** Modern (Dark/Light), Static (No Animation)
#     - **Tools:** Streamlit, LightGBM, Matplotlib, NumPy
#     """)
# 
# # Model Performance
# st.markdown("### üìä Model Performance")
# st.metric("Mean Squared Error (MSE)", value="131076370.20")
# st.metric("R¬≤ Score", value="0.84")
# 
# st.markdown("---")
# 
# # Input Form
# st.markdown("### ‚úèÔ∏è Enter Employee Details")
# with st.form("prediction_form"):
#     col1, col2 = st.columns(2)
#     with col1:
#         age = st.slider("Age", 18, 60, 30)
#         experience = st.slider("Experience (Years)", 0, 40, 5)
#         education = st.selectbox("Education", ["Bachelor", "Master", "PhD"])
#         location = st.selectbox("Location", ["Urban", "Suburban", "Rural"])
#     with col2:
#         job_title = st.selectbox("Job Title", ["Engineer", "Analyst", "Manager", "Technician"])
#         gender = st.radio("Gender", ["Male", "Female"])
#         hours = st.slider("Hours per Week", 20, 80, 40)
# 
#     submit = st.form_submit_button("üöÄ Predict Salary")
# 
# if submit:
#     # Placeholder input encoding
#     input_data = np.array([[age, experience, hours]])
#     prediction = model.predict(input_data)[0]
#     salary = f"‚Çπ{int(prediction):,}/month"
#     st.success(f"‚úÖ Estimated Salary: {salary}")
# # Predict and estimate salary
# if st.button("Estimate Salary"):
#     prediction = model.predict(input_data)
# 
#     if prediction[0] == 1:
#         salary = "‚Çπ75,000/month (High Income Group)"
#         salary_value = 75000
#     else:
#         salary = "‚Çπ30,000/month (Lower Income Group)"
#         salary_value = 30000
# 
#     st.success(f"Estimated Salary: {salary}")
# 
#     # Salary bar chart
#     st.write("### Salary Estimate Chart")
#     fig, ax = plt.subplots()
#     ax.bar(["Predicted Salary"], [salary_value], color="#1f77b4")
#     ax.set_ylabel("INR")
#     ax.set_ylim(0, 100000)
#     st.pyplot(fig)
#     # Footer
# st.markdown("---")
# st.markdown("#### üßë‚Äçüíª Developed by MUHILPIRASATH R | Sona College of Technology")
# st.markdown("</div>", unsafe_allow_html=True)
# from PIL import Image
# 
# st.set_page_config(page_title="Internship Project - Employee Salary Prediction", layout="wide")
# 
# # Header: Personal & Project Info
# st.markdown("<h1 style='text-align: center; color: #1f77b4;'>üéì Internship Project Showcase</h1>", unsafe_allow_html=True)
# st.markdown("---")
# 
# # Top row with image (optional)
# col1, col2 = st.columns([1, 3])
# with col1:
#     st.image("muhil.jpg", caption="MUHILPIRASATH R", width=180)  # Upload your image with this name
# with col2:
#     st.markdown("""
#         ### üë®‚Äçüíº **MUHILPIRASATH R**
#         üìò **College:** SONA COLLEGE OF TECHNOLOGY
#         üõ†Ô∏è **Branch:** Mechatronics Engineering ‚Äì Batch 2027
#         üè¢ **Company:** Edunet Foundation, AICTE & IBM
#         üìÖ **Duration:** 6 Weeks
#         üìå **Project:** Employee Salary Prediction using ML & DL
#     """)
#     st.image("ibm_logo.png", width=100)
#     st.image("badge.png", width=150)
# 
# st.markdown("---")
# 
# # Model Performance
# st.markdown("### üìä Model Performance Metrics")
# col1, col2 = st.columns(2)
# with col1:
#     st.metric(label="Mean Squared Error (MSE)", value="131076370.20")
# with col2:
#     st.metric(label="R¬≤ Score", value="0.84")
# 
# st.markdown("---")
# 
# # Developer intro
# st.markdown("### üë®‚Äçüíª Meet the Developer")
# st.markdown("""
# - üéì Pursuing 3rd Year B.E Mechatronics
# - üß† Passionate about AI, ML, Robotics & Automation
# - üîß Always exploring innovative solutions to real-world problems
# """)
# 
#

X = df.drop('income', axis=1)  # Replace 'target_column' with the actual target column name
y = df['income']  # Replace with your target column

# Save the trained model using joblib
joblib.dump(model, 'best_model_XGBoost.pkl')

from google.colab import files
# Download the model file to your local machine
files.download('best_model_XGBoost.pkl')

# Display results
results_df = pd.DataFrame(ml_results).T.sort_values(by="Accuracy", ascending=False)
print("\nML Model Performance Comparison:\n")
print(results_df)

# Ensure the models dictionary used here is the one containing all evaluated models (from cell n7DK5HpD8hot)
# This assumes the models dictionary from cell n7DK5HpD8hot is still available in the environment.
# If not, you might need to re-run cell n7DK5HpD8hot or redefine the models dictionary here.
models_with_xgboost = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC(),
    "Naive Bayes": GaussianNB(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "XGBoost": XGBClassifier(eval_metric='logloss')
}


# Step 1: Extract only accuracy
accuracy_results = {name: metrics["Accuracy"] for name, metrics in ml_results.items()}

# Step 2: Get best model name
best_model_name = max(accuracy_results, key=accuracy_results.get)

# Step 3: Get the corresponding model object from the correct 'models' dictionary
best_model = models_with_xgboost[best_model_name]

print(f"\n‚úÖ Best model: {best_model_name} with accuracy {accuracy_results[best_model_name]:.4f}")

# Step 4: Save the model
joblib.dump(best_model, f"best_model_{best_model_name}.pkl")
print(f"‚úÖ Saved best model as best_model_{best_model_name}.pkl")